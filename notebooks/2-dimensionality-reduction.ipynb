{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z96s2Yva82B-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for our visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXrbcy8L9Jki"
   },
   "outputs": [],
   "source": [
    "# Some utility functions for plotting\n",
    "\n",
    "def plot_eigenvectors(dataset, w, v):\n",
    "    plt.plot(dataset[\"x1\"], dataset[\"x2\"], \"bo\", markersize=5)\n",
    "    plt.arrow(0, 0, 10 * v[0, 0], 10 * v[1, 0], color=\"r\", linewidth=2, head_width=1, head_length=1)\n",
    "    plt.arrow( 0, 0, 10 * v[0, 1], 10 * v[1, 1], color=\"r\", linewidth=2, head_width=1, head_length=1)\n",
    "    plt.text( 12 * v[0, 0], 10 * v[1, 0], r\"PC1, $\\vec{v_1}$ =  %.2f $\\vec{x_1}$ + %.2f $\\vec{x_2}$\" % (v[0, 0], v[1, 0]), fontsize=15)\n",
    "    plt.text( 26 * v[0, 1], 8 * v[1, 1], r\"PC2, $\\vec{v_2}$ =  %.2f $\\vec{x_&}$ + %.2f $\\vec{x_2}$\" % (v[0, 1], v[1, 1]), fontsize=15)\n",
    "\n",
    "\n",
    "def plot_scatter_annotated(dataset, labels):\n",
    "    X = dataset.values\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=100)\n",
    "    for i, (x, y) in enumerate(zip(X[:, 0], X[:, 1])):\n",
    "        plt.annotate(\n",
    "            labels[i],\n",
    "            xy=(x, y),\n",
    "            xytext=(-20, 20),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.5),\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZq6177K82B8"
   },
   "source": [
    "# 2. Dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Theory behind PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is often useful for visualizing data and improving model performance by reducing overfitting and computational requirements. By removing noisy or correlated features, we can simplify the data while preserving as much relevant information as possible. When reducing the number of features to fewer than four, the data can often be visualized effectively, for example, in a scatterplot.\n",
    "\n",
    "In this context, we will first focus on **Principal Component Analysis (PCA)**, a popular **feature extraction** technique. PCA transforms the original data into a new set of features, known as principal components (PCs), which are orthogonal to each other and point in the direction of maximum variance.\n",
    "\n",
    "\n",
    "PCA decomposes a data set in its principal components (PC) through a **linear** transformation. PC's (also called eigenvectors) are **orthogonal** to each other and point in the direction of **largest variance**. So what does this mean? Let's say we have a data set with two features and we want to reduce this data to one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "dDHP7sjs82CE",
    "outputId": "a9700ead-8cef-4a20-a6cd-97fa985001fe"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/master/notebooks/8_dimensionality_reduction/pca.csv')\n",
    "sns.lmplot(x=\"x1\", y=\"x2\", data=dataset, fit_reg=False, height=8, scatter_kws={\"s\": 80})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMvl1tFw82CI"
   },
   "source": [
    "To apply PCA the data needs to be **centered** first. In this case it was centered already. PCA will search for the direction that preserves the most of the original variance. This is vector PC1 in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "id": "pZpDJ_Oz82CJ",
    "outputId": "72a07316-64e9-4aad-a563-705c2673a968"
   },
   "outputs": [],
   "source": [
    "w,v=np.linalg.eig(np.cov(dataset.values.T)) #finds the eigenvalues and principle components (eigenvectors)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_eigenvectors(dataset,w,v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second principal component (PC2) is orthogonal to the first (PC1). Together, PC1 and PC2 define a new space where PC1 captures most of the variance in the data. In general, for an $n$-dimensional dataset, PCA computes $n$ principal components, where each component captures the maximum variance while being orthogonal to the previous ones.\n",
    "\n",
    "Each $i$-th principal component has an **eigenvalue** $w_i$, which represents the amount of variance explained by that component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n6Ll9jF682CN",
    "outputId": "835fb147-e7a0-4b43-9a27-d87a92ca725b"
   },
   "outputs": [],
   "source": [
    "print(\"Eigenvalue for PC1 (w_0): {:.2f}\".format(w[0]))\n",
    "print(\"Eigenvalue for PC2 (w_1): {:.2f}\".format(w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6x96hBsH82CQ"
   },
   "source": [
    "The amount of variance explained by each PC $i$ is given by\n",
    "\n",
    "$$\\frac{w_i}{\\sum_j w_j},$$\n",
    "\n",
    "where $j$ ranges over all PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "c9JVM6nR82CR",
    "outputId": "96371281-f025-412d-e85a-8b7846efb187"
   },
   "outputs": [],
   "source": [
    "var_explained=100*w/sum(w)\n",
    "print(\"Variance explained by PC1: {}\".format(var_explained[0]))\n",
    "print(\"Variance explained by PC2: {}\".format(var_explained[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principal component (PC1) explains most of the variance in the original dataset. To reduce the number of dimensions, we can remove PC2 and project all data points onto PC1, which preserves 91.78% of the variance. In contrast, projecting onto PC2 alone preserves only 8.22%. Thus, to visualize an $n$-dimensional dataset, we can project it onto the top two or three principal components.\n",
    "\n",
    "For example, if we remove PC2 and project the data onto PC1, we can compute the new feature values from the original features $x_1$ and $x_2$ as:\n",
    "\n",
    "$$\\vec{v_1} =  0.85 \\vec{x_1} + 0.53 \\vec{x_2}.$$\n",
    "\n",
    "Here, we replace the original features $x_1$ and $x_2$ with the new feature $v_1$, which can then be plotted as a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "e3gvqZDJ82Cf",
    "outputId": "6cc0423a-5a15-461b-de72-341d487914a7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(v[0, 0] * dataset[\"x1\"] + v[1, 0] * dataset[\"x2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUkVD-gt82Cj"
   },
   "source": [
    "The variance of this distribution is thus 38.81. To show how PCA is applied using the PCA module in scikit-learn we first create an artificial data set with 100 samples in two classes. There are 100 features of which 10 are informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHujIwDN82Cj"
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "\n",
    "(dataset_big,targets) = datasets.make_classification(\n",
    "n_samples=100,\n",
    "n_features=100,\n",
    "n_classes=2,\n",
    "n_informative=10,\n",
    "n_redundant=0,\n",
    "n_repeated=0,\n",
    "class_sep=2,\n",
    "n_clusters_per_class=1,\n",
    "random_state=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this dataset is 100-dimensional, which makes it impossible to visualize directly.\n",
    "\n",
    "Now we apply PCA and keep only the two top principle components. The data set is projected (or transformed) onto these two new dimensions and plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "FGLD1jBi82Cn",
    "outputId": "593a5606-26f7-499e-e625-56ab40af6137"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "dataset_big_projected = pd.DataFrame(pca.fit(dataset_big).transform(dataset_big),\n",
    "                                     columns=['PC1','PC2'])\n",
    "dataset_big_projected['label'] = targets\n",
    "\n",
    "sns.lmplot(x=\"PC1\", y=\"PC2\", data=dataset_big_projected, hue='label', markers=['+','o'],\n",
    "           fit_reg=False, height=7, scatter_kws={\"s\": 80})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXoTWMSe82Cr"
   },
   "source": [
    "The plots reveals the two classes (remember these were not known to the PCA algorithm) and also reveals possible outliers in both classes. We can look at the explained variance for each PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Z90wRK1l82Cr",
    "outputId": "37e333a4-05cb-4ff2-c562-394def631671"
   },
   "outputs": [],
   "source": [
    "print(\"Variance explained by PC1: {}\".format(pca.explained_variance_ratio_[0]))\n",
    "print(\"Variance explained by PC2: {}\".format(pca.explained_variance_ratio_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe how PC1 and PC2 together only explain about 27% of the original variance while still providing useful structural information about the 100-dimensional data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Applying PCA, t-SNA, and UMAP to a biological dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "**Study:** *Feminizing gender-affirming hormone therapy remodels the plasma proteome*\n",
    "\n",
    "**Source:** \n",
    "- Article: [Nature Medicine (2025)](https://www.nature.com/articles/s41591-025-04023-9)\n",
    "- Data: [PRIDE Archive Project PAD000016](https://www.ebi.ac.uk/pride/archive/projects/PAD000016)\n",
    "\n",
    "**Abstract:**\n",
    "\n",
    "Sex differences manifest in various traits, as well as in the risk of cardiovascular, metabolic and immunological conditions. Despite the clear physical changes induced by gender-affirming hormone therapy (GAHT), little is known about how it affects underlying physiological and biochemical processes. Here we examined plasma proteome changes over 6 months of feminizing GAHT in 40 transgender individuals treated with estradiol plus one of two antiandrogens: cyproterone acetate or spironolactone. Testosterone levels dropped markedly in the cyproterone group, but less so in those receiving spironolactone. Among 5,279 total proteins measured, feminizing GAHT changed the levels of 245 and 91, in the cyproterone and spironolactone groups, respectively, with most (>95%) showing a decrease. Proteins associated with male spermatogenesis showed a marked decrease in the cyproterone group, attributable specifically to loss of testosterone. Changes in body fat percentage and breast volume following GAHT were also reflected in the plasma proteome, including an increase in leptin expression. We show that feminizing GAHT remodels the proteome toward a cis-female profile, altering 36 (cyproterone) and 22 (spironolactone) of the top 100 sex-associated proteins in UK Biobank adult data. Moreover, 43% of cyproterone-affected proteins overlapped with those altered by menopausal hormone therapy in cis women, showing the same directional changes, with notable exceptions including CXCL13 and NOS3. Feminizing GAHT skewed the protein profile toward that linked to asthma and autoimmunity, while GAHT with cyproterone specifically skewed it away from an atherosclerosis-associated profile, suggesting a protective effect. These results reveal that feminizing GAHT reshapes the plasma proteome in a hormone-dependent manner, with implications for reproductive capacity, immune regulation and long-term health outcomes.\n",
    "\n",
    "**Dataset Details:**\n",
    "\n",
    "- **Samples:** 113 plasma samples from transgender individuals undergoing feminizing GAHT, cis-female controls, cis-male controls, and pregnant individuals\n",
    "- **Features:** 5,440 proteins measured using proximity extension assay (PEA) technology\n",
    "- **Phenotype Groups:**\n",
    "  - SPIRO_GAHT (spironolactone + estradiol): baseline and 6-month timepoints\n",
    "  - CPA_GAHT (cyproterone acetate + estradiol): baseline and 6-month timepoints\n",
    "  - Control groups: cis-female and cis-male at baseline and follow-up timepoints\n",
    "  - Pregnancy: trimester 1 and trimester 3 samples\n",
    "- **Data Format:** Protein expression values are reported as PCNormalizedNPX values\n",
    "\n",
    "**Analysis Goal:**\n",
    "\n",
    "In this notebook, we apply three dimensionality reduction techniques—**PCA** (Principal Component Analysis), **t-SNE** (t-distributed Stochastic Neighbor Embedding), and **UMAP** (Uniform Manifold Approximation and Projection)—to visualize how the plasma proteome differs across phenotype groups and to identify key proteins driving these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_file = \"PAD000016_data.csv\"\n",
    "metadata_file = \"GAHT_pregnancy_metadata_SDRF.tsv\"\n",
    "\n",
    "# Read the CSV and TSV files\n",
    "data_df = # FILL IN\n",
    "metadata_df = # FILL IN\n",
    "\n",
    "# Extracting the phenotype labels from the metadata\n",
    "phenotype_labels = # FILL IN\n",
    "\n",
    "# Pivoting the data into wide format if needed (as in previous steps)\n",
    "data_wide = # FILL IN\n",
    "\n",
    "# Filling missing values with a set value\n",
    "data_wide_filled_missing_values = # FILL IN\n",
    "\n",
    "# Align phenotype labels with the sample_subset index\n",
    "# Match the metadata sample IDs with the data sample IDs\n",
    "phenotype_labels_aligned = metadata_df.set_index(\"source name\")[\"factor value[phenotype]\"].reindex(\n",
    "    data_wide_filled_missing_values.index, fill_value=\"Unknown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 - Do we need the labels for dimensionality reduction?\n",
    "\n",
    "Question 2 - Here we do not normalize the data, is normalization neccesary? If yes, why? If yes, what kind of normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will fit the models to perform the dimensionality reduction. You can always look at the documentation to see how the models should be fitted [scikit-learn](https://scikit-learn.org/stable/) and [UMAP](https://umap-learn.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = # FILL IN\n",
    "pca_result = # FILL IN\n",
    "\n",
    "# Perform t-SNE with parallelization and adjusted perplexity for performance\n",
    "tsne = # FILL IN\n",
    "tsne_result = # FILL IN\n",
    "\n",
    "# Perform UMAP (ensure umap-learn is installed)\n",
    "umap_model = # FILL IN\n",
    "umap_result = # FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the results from the different dimensionality reduction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PCA, t-SNE, and UMAP results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "# PCA plot\n",
    "sns.scatterplot(\n",
    "    x=pca_result[:, 0],\n",
    "    y=pca_result[:, 1],\n",
    "    hue=phenotype_labels_aligned,\n",
    "    ax=axes[0],\n",
    "    palette=\"viridis\",\n",
    "    legend=False,\n",
    ")\n",
    "axes[0].set_title(\"PCA\")\n",
    "\n",
    "# t-SNE plot\n",
    "sns.scatterplot(\n",
    "    x=tsne_result[:, 0],\n",
    "    y=tsne_result[:, 1],\n",
    "    hue=phenotype_labels_aligned,\n",
    "    ax=axes[1],\n",
    "    palette=\"viridis\",\n",
    "    legend=False,\n",
    ")\n",
    "axes[1].set_title(\"t-SNE\")\n",
    "\n",
    "# UMAP plot\n",
    "sns.scatterplot(\n",
    "    x=umap_result[:, 0],\n",
    "    y=umap_result[:, 1],\n",
    "    hue=phenotype_labels_aligned,\n",
    "    ax=axes[2],\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "axes[2].set_title(\"UMAP\")\n",
    "\n",
    "# Move legend outside the plot area to the right\n",
    "axes[2].legend(\n",
    "    bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0, frameon=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 - What are the differences between the different algorithms? Are the differences you see expected for PCA/t-SNE/UMAP?\n",
    "\n",
    "Question 4 - What could this visualization be useful for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different perplexity values\n",
    "perplexity_values = # FILL IN\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(perplexity_values) / 3), 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, perp in enumerate(perplexity_values):\n",
    "    # Run t-SNE with different perplexity\n",
    "    tsne_temp = # FILL IN\n",
    "    tsne_temp_result = # FILL IN\n",
    "\n",
    "    # Scatter plot of t-SNE\n",
    "    sns.scatterplot(\n",
    "        x=tsne_temp_result[:, 0],\n",
    "        y=tsne_temp_result[:, 1],\n",
    "        hue=phenotype_labels_aligned,\n",
    "        ax=axes[idx],\n",
    "        palette=\"viridis\",\n",
    "        legend=(idx == 0),\n",
    "        s=60,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    axes[idx].set_title(\n",
    "        f\"t-SNE with Perplexity = {perp}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"t-SNE Dimension 1\", fontsize=11)\n",
    "    axes[idx].set_ylabel(\"t-SNE Dimension 2\", fontsize=11)\n",
    "\n",
    "    if idx == 0:\n",
    "        axes[idx].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 - How do different perplexity values have an effect on the dimensionality reduction?\n",
    "\n",
    "Question 6 - How do you select an appropiate perplexity value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances and Interpretation\n",
    "\n",
    "For PCA, we can examine the **loadings** (also called components or feature weights) which tell us how much each original feature (protein) contributes to each principal component. Features with large absolute loadings have the most influence on that PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Feature Importances (Loadings)\n",
    "# Get the loadings for PC1 and PC2\n",
    "loadings = pd.DataFrame(\n",
    "    # FILL IN\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=# FILL IN,\n",
    ")\n",
    "\n",
    "# Calculate the absolute importance (magnitude of loading)\n",
    "loadings[\"PC1_abs\"] = np.abs(loadings[\"PC1\"])\n",
    "loadings[\"PC2_abs\"] = np.abs(loadings[\"PC2\"])\n",
    "\n",
    "# Get top 15 features for each PC\n",
    "top_pc1 = loadings.nlargest(# FILL IN, \"PC1_abs\")[[\"PC1\"]].sort_values(\"PC1\")\n",
    "top_pc2 = loadings.nlargest(# FILL IN, \"PC2_abs\")[[\"PC2\"]].sort_values(\"PC2\")\n",
    "\n",
    "print(\n",
    "    \"Variance explained by PC1: {:.2f}%\".format(pca.explained_variance_ratio_[0] * 100)\n",
    ")\n",
    "print(\n",
    "    \"Variance explained by PC2: {:.2f}%\".format(pca.explained_variance_ratio_[1] * 100)\n",
    ")\n",
    "print(\n",
    "    \"Total variance explained for the top 15: {:.2f}%\".format(sum(pca.explained_variance_ratio_) * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7 - Is the variance explained by PC1 large when considering the number of features? Is there a steep drop-off in variance explained? Is this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top feature loadings for PC1 and PC2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PC1 loadings\n",
    "top_pc1.plot(kind=\"barh\", ax=axes[0], color=\"steelblue\", legend=False)\n",
    "axes[0].set_title(\"Top 15 Feature Loadings for PC1\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Loading Value\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Protein/Feature\", fontsize=12)\n",
    "axes[0].axvline(x=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "# PC2 loadings\n",
    "# FILL IN\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8 - What are the top 5 most important proteins for PC1 and PC2, respectively?\n",
    "\n",
    "Question 9 - Can the loadings between different principal components overlap? What does it indicate if a protein has a high loading for the first couple of components? How can we account for this spread of loadings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 feature names for PC1\n",
    "top_5_pc1_features = # FILL IN\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_5_pc1_features):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Scatter plot: Feature vs PC1\n",
    "    scatter = ax.scatter(\n",
    "        # FILL IN,\n",
    "        pca_result[:, 0],\n",
    "        c=pd.Categorical(phenotype_labels_aligned).codes,\n",
    "        cmap=\"viridis\",\n",
    "        s=60,\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # Calculate correlation\n",
    "    corr = # FILL IN https://numpy.org/doc/2.3/reference/generated/numpy.corrcoef.html\n",
    "\n",
    "    # Add regression line using sklearn\n",
    "    X = data_wide_filled_missing_values[feature].values.reshape(-1, 1)\n",
    "    y = pca_result[:, 0]\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "\n",
    "    line_x = np.array(\n",
    "        [\n",
    "            data_wide_filled_missing_values[feature].min(),\n",
    "            data_wide_filled_missing_values[feature].max(),\n",
    "        ]\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    line_y = reg.predict(line_x)\n",
    "\n",
    "    ax.plot(line_x, line_y, \"r--\", linewidth=2, alpha=0.7)\n",
    "\n",
    "    # Formatting\n",
    "    feature_name = feature.replace(\"_\", \" \")\n",
    "    ax.set_xlabel(f\"{feature_name} Expression\", fontsize=10)\n",
    "    ax.set_ylabel(\"PC1 Score\", fontsize=10)\n",
    "    ax.set_title(\n",
    "        f'{feature_name}\\nLoading={loadings.loc[feature, \"PC1\"]:.4f}, r={corr:.3f}',\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10 - Does a high loading correlate with a good split between the classes? Why not? why yes?\n",
    "\n",
    "Question 11 - Investigate some of the proteins and their implications on pregnancy and hormone therapy, are there any relations?\n",
    "\n",
    "Questions 12 - What would be a next step in data analysis after reducing the dimensions?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "8._Dimensionality_reduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
